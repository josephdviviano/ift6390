{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 4: KNN, Training and Test set, Decision boundaries. 20/09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week you implemented the 1 nearest neighbor (1NN) algorithm. This week you will implement the KNN algorithm. On top of this, we will also touch upon training and test sets, as well as decision boundary.\n",
    "\n",
    "- Your first step is to remember how KNN works.\n",
    "- We provide you with the boilerplate code in which you will need to insert your KNN implementation. This way you will be able to focus on the actual algorithm itself and not have to worry about such things as printing or displaying results.\n",
    "- You can execute each cell by clicking on Cell/Run all: On the last cell you will observe a constant classifier (which also predicts, regardless of its input, class 1 (blue))\n",
    "\n",
    "Familiarize yourself with the code in the following 5 sections:\n",
    "   - **Utility Functions:** Defines helper functions such as for visualization, evaluation, etc.\n",
    "   - **KNN class:** This is where you will implement the classifier.\n",
    "   - **Loading and splitting the data:** Loads a dataset and splits it into two parts (train, test)\n",
    "   - **Initialization and training of classifier:** Trains a KNN model on the train dataset and obtains predictions on the test set\n",
    "   - **Confusion matrix and decision boundary:** Displays the confusion matrix and visualizes the decision boundary of our trained classifier\n",
    "\n",
    "**Your objective for this demo session** is to understand the general functioning of the code below and then to fill in the function knn.compute_predictions()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we will implement KNN as a **class**. You can read this [tutorial](http://docs.python.org/2/tutorial/classes.html) if you are not familiar with the concept of classes in Object-oriented programming or its syntax in python. The class `knn` is already partially implemented. All that you have left to do is to write the method `compute_predictions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have anything to implement here. Simply read the code and familiarize yourelf with it. You will be able to test the functions `test` and `gridplot` at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import random\n",
    "import pylab\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions calculates the Minkowski distance between a vector x and a matrix Y. Does this remind you of anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minkowski_mat(x,Y,p=2):\n",
    "    return (np.sum((np.abs(x-Y))**p,axis=1))**(1.0/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `conf_matrix` takes as input:\n",
    "\n",
    "- `testlabels` - test labels\n",
    "- `predlabels` - prediction labels\n",
    "and returns a table presenting the results\n",
    "\n",
    "See the definition of [Confusion matrix](http://fr.wikipedia.org/wiki/Matrice_de_confusion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(testlabels, predlabels):\n",
    "\n",
    "\tn_classes = int(max(testlabels))\n",
    "\tmatrix = np.zeros((n_classes,n_classes))\n",
    "\n",
    "\tfor (test,pred) in zip(testlabels, predlabels):\n",
    "\t\tmatrix[test-1,pred-1] += 1\n",
    "\n",
    "\treturn matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `gridplot` takes as input:\n",
    "\n",
    "- `classifier` - a classifier such as `knn`\n",
    "- `train` - a training set\n",
    "- `test` - a test set\n",
    "- `n_points` - the width/height of the grid on which to visualize the decision boundary (n,n)\n",
    "\n",
    "Depending on the speed of your computer, calculating of predictions on the grid can be slow. We recommend doing the first tests with a small grid (say, 25 by 25). You could then augment the size of the grid to 50x50 or even 100x100 to obtain better looking visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function plot\n",
    "def gridplot(classifier,train,test,n_points=50):\n",
    "\n",
    "    train_test = np.vstack((train,test))\n",
    "    (min_x1,max_x1) = (min(train_test[:,0]),max(train_test[:,0]))\n",
    "    (min_x2,max_x2) = (min(train_test[:,1]),max(train_test[:,1]))\n",
    "\n",
    "    xgrid = np.linspace(min_x1,max_x1,num=n_points)\n",
    "    ygrid = np.linspace(min_x2,max_x2,num=n_points)\n",
    "\n",
    "\t# calculates the cartesian product between two lists\n",
    "    # and stores the result in an array\n",
    "    thegrid = np.array(combine(xgrid,ygrid))\n",
    "\n",
    "    counts = classifier.compute_predictions(thegrid)\n",
    "    predictedClasses = np.argmax(counts,axis=1)+1\n",
    "\n",
    "    # The grid\n",
    "    # To get a better looking grid:\n",
    "    #props = dict( alpha=0.3, edgecolors='none' )\n",
    "    #pylab.scatter(thegrid[:,0],thegrid[:,1],c = classesPred, s=50, edgecolors='none')\n",
    "    pylab.pcolormesh(xgrid, ygrid, predictedClasses.reshape((n_points, n_points)).T, alpha=.3)\n",
    "\t# Training data points\n",
    "    pylab.scatter(train[:,0], train[:,1], c = train[:,-1], marker = 'v', s=150)\n",
    "    # Test data points\n",
    "    pylab.scatter(test[:,0], test[:,1], c = test[:,-1], marker = 's', s=150)\n",
    "\n",
    "    ## A hack, since pylab is lacking this functionality... :(\n",
    "    h1 = pylab.plot([min_x1], [min_x2], marker='o', c = 'w',ms=5) \n",
    "    h2 = pylab.plot([min_x1], [min_x2], marker='v', c = 'w',ms=5) \n",
    "    h3 = pylab.plot([min_x1], [min_x2], marker='s', c = 'w',ms=5) \n",
    "    handles = [h1,h2,h3]\n",
    "    ## End of hack :)\n",
    "\n",
    "    labels = ['grid','train','test']\n",
    "    pylab.legend(handles,labels)\n",
    "\n",
    "    pylab.axis('equal')\n",
    "    pylab.show()\n",
    "    \n",
    "## http://code.activestate.com/recipes/302478/\n",
    "def combine(*seqin):\n",
    "    '''returns a list of all combinations of argument sequences.\n",
    "for example: combine((1,2),(3,4)) returns\n",
    "[[1, 3], [1, 4], [2, 3], [2, 4]]'''\n",
    "    def rloop(seqin,listout,comb):\n",
    "        '''recursive looping function'''\n",
    "        if seqin:                       # any more sequences to process?\n",
    "            for item in seqin[0]:\n",
    "                newcomb=comb+[item]     # add next item to current comb\n",
    "                # call rloop w/ rem seqs, newcomb\n",
    "                rloop(seqin[1:],listout,newcomb)\n",
    "        else:                           # processing last sequence\n",
    "            listout.append(comb)        # comb finished, add to list\n",
    "    listout=[]                      # listout initialization\n",
    "    rloop(seqin,listout,[])         # start recursive process\n",
    "    return listout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `knn` takes as parameters:\n",
    "\n",
    "- `n_classes` - the number of classes in the problem\n",
    "- `dist_func` - a function to calculate the distance between points\n",
    "- `n_voisins` - the number of neighbors to visit\n",
    "\n",
    "The method `train` is actually really only storing the dataset. All of the work is done at prediction time for `knn` models.\n",
    "\n",
    "The method `compute_predictions` takes as input the unlabeled test set in matrix form and returns the matrix of counts for each test set example. This matrix is hence of dimensions (n_example, n_classes).\n",
    "\n",
    "You will need to for each test set example:\n",
    "\n",
    " - **calculate distances** for every point of the training set (using dist_func)\n",
    " - Look through the distances to **find the $k$ nearest neighbors** of the current test example\n",
    " - **Calculate the number of neighbors per class** and save them in `counts`\n",
    " \n",
    " **Note :** `knn.compute_predictions()`'s output needs to be general enough to be used in different contexts. This is why we ask that it returns a matrix containing the counts for each example in the test set and not the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self,n_classes, dist_func=minkowski_mat, nn=1):\n",
    "        self.n_classes = n_classes\n",
    "        self.dist_func = dist_func\n",
    "        self.nn = nn\n",
    "\n",
    "    # The train function for knn is really only storing the dataset\n",
    "    def train(self, train_inputs, train_labels):\n",
    "        self.train_inputs = train_inputs\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "    ###\n",
    "    # The prediction function takes as input:\n",
    "    #   test_data - Unlabeled test data points\n",
    "    # and returns a matrix containing counts for each test set example. \n",
    "    # Each row of this matrix contains, for each class, the number of \n",
    "    # neighbors belonging to this class. \n",
    "    ###\n",
    "    def compute_predictions(self, test_data):\n",
    "        # Initialization of the matrix to return\n",
    "        num_test = test_data.shape[0]\n",
    "        counts = np.ones((num_test,self.n_classes))\n",
    "\n",
    "        # For each test datapoint\n",
    "        for (i,ex) in enumerate(test_data):\n",
    "            # Comment after having implemented the function\n",
    "            pass\n",
    "\n",
    "            # i is the row index\n",
    "            # ex is the i'th row\n",
    "\n",
    "            # Find the distances to each training set point\n",
    "            # using dist_func\n",
    "            distances = self.dis_func()\n",
    "            \n",
    "            # Go through the training set to find the \n",
    "            # neighbors of the current point (ex)\n",
    "            # ---> Write code here\n",
    "\n",
    "            # Calculate the number of neighbors belonging to each class\n",
    "            # and write them in counts[i,:]\n",
    "            # ---> Write code here\n",
    "\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `iris` dataset is divided into two parts, one for training and the other for testing.\n",
    "It is important to shuffle randomly the dataset before splitting it. Can you tell why?\n",
    "\n",
    "Only two columns of the dataset are used to visualize them in 2-dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris\n",
    "iris = np.loadtxt('iris.txt')\n",
    "data = iris\n",
    "\n",
    "# Number of classes\n",
    "n_classes = 3\n",
    "# Size of training set\n",
    "n_train = 100\n",
    "\n",
    "# The columns (features) on which to train our model\n",
    "# For gridplot to work, len(train_cols) should be 2\n",
    "train_cols = [0,1]\n",
    "# The index of the column containing the labels\n",
    "target_ind = [data.shape[1] - 1]\n",
    "\n",
    "# Comment to have random (non-deterministic) results \n",
    "random.seed(3395)\n",
    "# Randomly choose indexes for the train and test dataset\n",
    "inds = list(range(data.shape[0]))\n",
    "random.shuffle(inds)\n",
    "train_inds = inds[:n_train]\n",
    "test_inds = inds[n_train:]\n",
    "\n",
    "# Split the data into both sets\n",
    "train_set = data[train_inds,:]\n",
    "train_set = train_set[:,train_cols + target_ind]\n",
    "test_set = data[test_inds,:]\n",
    "test_set = test_set[:,train_cols + target_ind]\n",
    "\n",
    "# Separate the test set into inputs and labels\n",
    "test_inputs = test_set[:,:-1]\n",
    "test_labels = test_set[:,-1].astype('int32')\n",
    "train_inputs = train_set[:, :-1]\n",
    "train_labels = train_set[:, -1].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and training of the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the argmax (class with most votes) of the predictions for each testset example to get a prediction.\n",
    "\n",
    "Don't forget to rerun this cell if you have modified your model and would like to display the decision boundary in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of neighbors (k) for knn\n",
    "k = 2\n",
    "print(\"We will train \",k, \"-NN on \", n_train, \" training set examples\")\n",
    "\n",
    "# Create the classifier\n",
    "model = knn(n_classes,dist_func = minkowski_mat, nn = k)\n",
    "# We train the model\n",
    "model.train(train_inputs, train_labels)\n",
    "# We get predictions\n",
    "t1 = time.clock()\n",
    "counts = model.compute_predictions(test_inputs)\n",
    "t2 = time.clock()\n",
    "print('It took ', t2-t1, ' seconds to calculate the predictions on ', test_inputs.shape[0],' test set examples')\n",
    "\n",
    "# Majority vote (+1 since our classes are labeled from 1 to n)\n",
    "classes_pred = np.argmax(counts,axis=1)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix and decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print the confusion matrix, which is very useful for analyzing which classes our classifier is having a hard time predicting. We also create a graph displaying the training points as well as the test points and the decision boundary of our model.\n",
    "\n",
    "Before moving on to the next section, please make sure that your $k$-nn implementation works well by executing this code. Do not hesitate to ask questions if you have trouble interpreting the confusion matrix or the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests\n",
    "# Confusion Matrix \n",
    "confmat = conf_matrix(test_labels, classes_pred)\n",
    "print('The confusion matrix is:')\n",
    "print(confmat)\n",
    "\n",
    "# Test error\n",
    "sum_preds = np.sum(confmat)\n",
    "sum_correct = np.sum(np.diag(confmat))\n",
    "print(\"The test error is \", 100*(1.0 - (float(sum_correct) / sum_preds)),\"%\")\n",
    "\n",
    "# The grid size will be = grid_size x grid_size\n",
    "grid_size = 200\n",
    "\n",
    "if len(train_cols) == 2:\n",
    "    # Decision boundary\n",
    "    t1 = time.clock()\n",
    "    gridplot(model, train_set, test_set, n_points = grid_size)\n",
    "    t2 = time.clock()\n",
    "    print('It took ', t2-t1, ' seconds to calculate the predictions on', grid_size * grid_size, ' points of the grid')\n",
    "    filename = 'grid_' + '_k=' + str(k) + '_c1=' + str(train_cols[0]) + '_c2=' + str(train_cols[1])+'.png'\n",
    "    print('We will save the plot into {}'.format(filename))\n",
    "    pylab.savefig(filename,format='png')\n",
    "else:\n",
    "    print('Too many dimensions (', len(train_cols),') to print the decision boundary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is working properly, it is time to play with the model in order to better understand the different parameters. Work directly with the code above to run these tests.\n",
    "\n",
    "- Vary the size of `train_set` and `test_set` and observe the impact that it has on the test error and the decision boundary\n",
    "- Try $k=1,2,\\dots,10$.\n",
    "  - Does the test error change?\n",
    "  - Is there an optimal $k$?\n",
    "  - Are you able to tell which $k$ is optimal only by looking at the decision boundary?\n",
    "- Divide the training set into 3 parts: `train_set`, `valid_set` and `test_set` (of size 100, 25 and 25, for example). Train $k$-nn on `train_set`, then choose the optimal $k$ using the `valid_set` and finally obtain an estimate of the generalization error of your model by testing on `test_set`. This time, use all 4 features of the dataset. What do you think the validation set is used for?\n",
    "  - Is there a difference between the validation error and the test error for the optimal $k$ found using the validation set? Should there be? (the answer can be found in the question)\n",
    "- Uncomment the line `random.seed(3395)` and run your code multiple times to get statistics on the validation and test errors. You can write a `for` loop to execute the same piece of code multiple times; 10 times should be enough. Calculate the mean and standard deviation for each error.\n",
    "\n",
    "Do not hesitate to validate your answers by asking questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
